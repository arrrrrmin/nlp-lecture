{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa411d5-4427-4eaf-b47f-035fe5be4dae",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c6009-98d0-45df-a17f-9f19eac68714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb907b0-79ff-4651-b1bc-b033df3d0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".strip(\".\").strip(\",\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eae7a7-1b5e-4eb4-9a14-b2493aa5e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(context_size, len(raw_text) - context_size):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(context_size)]\n",
    "        + [raw_text[i + j + 1] for j in range(context_size)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be79eb2-62af-4752-8901-dbfc17defbf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c126a-279d-4d44-bfe7-3f26a23f30f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, context_size: int, embedding_dim: int):\n",
    "        # self.input_size = context_size * 2  # To the left and right\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.projection1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.projection2 = nn.Linear(128, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        e = self.embed(inputs).flatten(1, 2)\n",
    "        p1 = self.activation(self.projection1(e))\n",
    "        p2 = self.projection2(p1)\n",
    "        \n",
    "        return self.softmax(p2)\n",
    "\n",
    "def make_context_vector(context, word_to_idx):\n",
    "    idxs = [word_to_idx[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_idx)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d4b6d-2250-4cbc-a80d-a908bcd49f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size, context_size, 64)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa003aaf-df08-4ee9-a0ff-5dc91d06ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data[0]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941aa313-4d8b-4ce3-b77b-f55f280799be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(500):\n",
    "    total_loss = 0\n",
    "    for d in data:\n",
    "        model.zero_grad()\n",
    "        x, y_true = d\n",
    "        y_hat = model(make_context_vector(x, word_to_idx).unsqueeze(0))\n",
    "\n",
    "        loss = loss_function(y_hat, torch.tensor([word_to_idx[y_true]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        total_loss += loss.detach().item()\n",
    "        optimizer.step()\n",
    "    losses.append(total_loss / len(data))   \n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1c41e-0b02-4bcc-9027-daa3980f1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf45b5-bdb8-43fa-9960-ff36bd81b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionResult = tuple[torch.Tensor, tuple[str, torch.Tensor, torch.Tensor]]  # (word, word_idx, pred_loss)\n",
    "\n",
    "def predict(cbow_model: CBOW, input: torch.Tensor, top_n: int = 3):\n",
    "    cbow_model.eval()\n",
    "    context_idxs = torch.tensor([word_to_idx[w] for w in input], dtype=torch.long).unsqueeze(0)\n",
    "    res = cbow_model.forward(context_idxs)\n",
    "    y_hat = torch.argmax(res)\n",
    "    res_val, res_ind = res.sort(descending=True)\n",
    "    res_val = res_val[0][:top_n]\n",
    "    res_ind = res_ind[0][:top_n]\n",
    "    top_n_ranked = [(idx_to_word[ind.item()], ind, val.detach()) for ind, val in zip(res_ind, res_val)]\n",
    "    \n",
    "    return y_hat, top_n_ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4ba41-93f0-4aa4-b417-41b51f44e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a word given some context\n",
    "x_words = \"We are to study\".split()\n",
    "y_word = \"about\"\n",
    "n = 3\n",
    "\n",
    "y_hat, top_n = predict(model, x_words, top_n=n)\n",
    "y_true = torch.tensor([word_to_idx[y_word]], dtype=torch.long)\n",
    "\n",
    "print(y_hat)\n",
    "print(y_true)\n",
    "print(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14baff4-7716-4086-adc5-849555f90e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(data[len(data)-10:]):\n",
    "    x_words, y_word = d\n",
    "    y_hat, top_n = predict(model, x_words)\n",
    "    y_true = torch.tensor([word_to_idx[y_word]], dtype=torch.long)\n",
    "    print(f\"Example {i}, Top {n} predictions\", top_n)\n",
    "    print(f\"Target word '{y_word}' \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
